{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4409102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/jsoelter/Code/ChestImageAI/utils/')\n",
    "sys.path.append('/home/users/jsoelter/Code/big_transfer/')\n",
    "\n",
    "import data_loader, evaluations, model_setup, sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81320d7",
   "metadata": {},
   "source": [
    "### Parameter managment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053427b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "defaults = {} # default parameter, will be overwritten if set explicilty in this notebooke\n",
    "overwrites = {} # parameter that OVERWRITE all parameters set in this notebook. Usefull if Notebook is executed externally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30acacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sacred.ParameterStore(overwrites=overwrites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc6508",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.model_dict =  dict(\n",
    "    architecture = 'BiT-M-R50x3',\n",
    "    #architecture = 'densenet121',\n",
    "    num_classes = 1,\n",
    "    #pretrained = 'imagenet', \n",
    "    pretrained = '/home/users/jsoelter/models/chexpert/fullmeta_503_consolidation_new/step00200.pt', #None, #'imagenet','imagenet', #\n",
    "    fresh_head_weights = True,\n",
    "    num_meta=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2217d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.computation = {\n",
    "    'model_out': '/home/users/jsoelter/models/rsna/bitm/new_exp/test',\n",
    "    'device': \"cuda:3\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(p.computation['model_out']):\n",
    "    os.makedirs(p.computation['model_out'])\n",
    "\n",
    "model = model_setup.instantiate_model(**p.model_dict)\n",
    "\n",
    "saved_models = glob.glob(os.path.join(p.computation['model_out'], 'step*.pt'))\n",
    "if not saved_models:\n",
    "    checkpoint = None\n",
    "    ledger = collections.defaultdict(list)\n",
    "    step = 0\n",
    "else:\n",
    "    last_model = np.sort(saved_models)[-1]\n",
    "    print(f\"Resume training for saved model '{last_model}'\")\n",
    "    checkpoint = torch.load(last_model, map_location=\"cpu\")\n",
    "    re_keyed = {k.split('module.')[-1]: v for k, v in checkpoint['model'].items()}\n",
    "    model.load_state_dict(re_keyed)\n",
    "    \n",
    "    ledger = json.load(open(os.path.join(p.computation['model_out'], 'train_ledger.json')))\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    \n",
    "# Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "# Only good if sizes stay the same within the main loop!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#model = torch.nn.DataParallel(model)\n",
    "device = p.computation['device']\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0901fe4",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.data_setup = dict(\n",
    "    data =  dict(\n",
    "        include_meta = [],\n",
    "        #subset = {'Sex': ['M']}\n",
    "        include_meta_features = [], #['Sex'],\n",
    "        #include_meta = ['Sex', 'AP/PA', 'Frontal/Lateral'],\n",
    "        val_conf = {\n",
    "                'salt': '42',\n",
    "                'fraction': 0.05,\n",
    "            }\n",
    "    ),\n",
    "    transforms = [\n",
    "        ('ToPILImage', {}),\n",
    "        ('Resize', {\n",
    "            'size': 256+32 #smaller edege mapped to x\n",
    "        }),\n",
    "        #('Resize', {\n",
    "        #    'size': 544\n",
    "        #}),\n",
    "        ('RandomRotation', {\n",
    "            'degrees': 5\n",
    "        }),    \n",
    "        ('RandomCrop', {\n",
    "            'size': (256, 256)\n",
    "        }),\n",
    "        ('ToTensor', {}),\n",
    "        ('Normalize', {\n",
    "            #'mean': [0.485, 0.456, 0.406], \n",
    "            'mean': (0.5, 0.5, 0.5),\n",
    "            #'std': [0.229, 0.224, 0.225]  \n",
    "            'std': (0.5, 0.5, 0.5)\n",
    "        }),\n",
    "])\n",
    "\n",
    "p.sampling_config = dict(\n",
    "    meta_field = 'Sex',\n",
    "    meta_values = ['M', 'F'],\n",
    "    frac_meta0 = 0.5,\n",
    "    frac_meta0_tar1 = 0,\n",
    "    frac_meta1_tar1 = 0.5,\n",
    "    max_samples = 15000\n",
    ")\n",
    "\n",
    "sampling_config2 = p.sampling_config.copy()\n",
    "sampling_config2['frac_meta0'] = 0.5\n",
    "sampling_config2['frac_meta0_tar1'] = 0.2\n",
    "sampling_config2['frac_meta1_tar1'] = 0.2\n",
    "\n",
    "p.sampling_config2 = sampling_config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = data_loader.transform_pipeline_from_dict(p.data_setup['transforms'])\n",
    "\n",
    "train_data = data_loader.RSNAPneumoniaData(\n",
    "    transform=preprocess, \n",
    "    sub_sampling=p.sampling_config, \n",
    "    validation=False, \n",
    "    datapath = '/work/projects/covid19_dv/raw_data/rsna_pneunomia/',                                       \n",
    "    **p.data_setup['data']\n",
    ")\n",
    "\n",
    "valid_data = data_loader.RSNAPneumoniaData(\n",
    "    transform=preprocess,\n",
    "    sub_sampling=p.sampling_config,\n",
    "    validation=True, \n",
    "    datapath = '/work/projects/covid19_dv/raw_data/rsna_pneunomia/',                                       \n",
    "    **p.data_setup['data'])\n",
    "\n",
    "valid_data2 = data_loader.RSNAPneumoniaData(\n",
    "    transform=preprocess,\n",
    "    sub_sampling=p.sampling_config2,\n",
    "    validation=True, \n",
    "    datapath = '/work/projects/covid19_dv/raw_data/rsna_pneunomia/',                                       \n",
    "    **p.data_setup['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25039398",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch_size = 128 #\n",
    "batch_split = 8 #number of forward pathes before optimization is performed \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True, drop_last=False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=16, num_workers=8)\n",
    "valid_loader2 = torch.utils.data.DataLoader(valid_data2, batch_size=16, num_workers=8)\n",
    "\n",
    "steps_per_epoch = int(len(train_loader)/batch_split)\n",
    "print(f'steps per epoch: {steps_per_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1e234",
   "metadata": {},
   "source": [
    "### Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = 'Adam' #'Adam'#\n",
    "p.opt = {\n",
    "    'class': 'SGD',\n",
    "    'param': dict(\n",
    "        lr = 2E-4,\n",
    "        momentum=0.9,\n",
    "        nesterov = True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14212c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = getattr(torch.optim, p.opt['class'])(model.parameters(), **p.opt['param'])\n",
    "if  checkpoint is not None:\n",
    "    optim.load_state_dict(checkpoint[\"optim\"])\n",
    "else:\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.scheduler = dict(\n",
    "    supports = [int(0.5*steps_per_epoch), 1*steps_per_epoch, 2*steps_per_epoch, 3*steps_per_epoch, 4*steps_per_epoch]\n",
    ")\n",
    "#supports = [2*steps_per_epoch, 3*steps_per_epoch, 4*steps_per_epoch, 6*steps_per_epoch, 8*steps_per_epoch]#[3000, 7000, 9000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21f25e",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad62f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.loss_param = {\n",
    "    #'pos_weight': 2.3\n",
    "}\n",
    "\n",
    "crit = model_setup.maskedBCE(**p.loss_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69462a",
   "metadata": {},
   "source": [
    "### Initial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c460ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = evaluations.batch_prediction(model, valid_loader, device=device)\n",
    "print(f'AUC: {evaluations.eval_auc(preds.reshape(-1, 1), targets):.3f}')\n",
    "print(f'Crit: {evaluations.eval_crit(model, valid_loader, crit, device=device):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5857ae7",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intervall = 20\n",
    "save_intervall = 500#steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steps = 0\n",
    "batch_loss, batch_samples = 0, 0\n",
    "lr = p.opt['param']['lr']\n",
    "\n",
    "train_setup = ledger.setdefault('train_setup', {})\n",
    "train_setup[step] = {\n",
    "    'setup': p.params,\n",
    "    'real_batch_size': real_batch_size,\n",
    "    'batch_split': batch_split\n",
    "}\n",
    "\n",
    "while lr:\n",
    "    for x, y, m in train_loader:\n",
    "        \n",
    "        _ = model.train()\n",
    "\n",
    "        # Update learning-rate, including stop training if over.\n",
    "        lr = model_setup.get_lr(step, supports=p.scheduler['supports'], base_lr=p.opt['param']['lr'])\n",
    "        if lr is None: break\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        \n",
    "        # Schedule sending to GPU(s)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        if getattr(model, 'meta_injection', None):\n",
    "            m = m.to(device, non_blocking=True)\n",
    "            logits = model(x, m)\n",
    "        else:\n",
    "            logits = model(x)            \n",
    "        loss, n_samples = crit(logits, y)\n",
    "        if loss != 0:\n",
    "            # Accumulate grads\n",
    "            (loss / batch_split / n_samples).backward()\n",
    "\n",
    "        batch_loss += float(loss.data.cpu().numpy())  # Also ensures a sync point.\n",
    "        batch_samples += n_samples.cpu().numpy()\n",
    "\n",
    "        accum_steps += 1\n",
    "\n",
    "        # Update params\n",
    "        if accum_steps == batch_split:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            train_loss = batch_loss/batch_samples\n",
    "            ledger['train_loss'].append(train_loss)\n",
    "            batch_loss, batch_samples = 0, 0\n",
    "            ledger['lr'].append(lr)\n",
    "            step += 1\n",
    "            accum_steps = 0\n",
    "            \n",
    "            # Evaluate \n",
    "            if (step % eval_intervall) == 0:\n",
    "                preds, targets = evaluations.batch_prediction(model, valid_loader, device=device)\n",
    "                auc_selection = evaluations.eval_auc(preds.reshape((-1,1)), targets)\n",
    "                preds, targets = evaluations.batch_prediction(model, valid_loader2, device=device)\n",
    "                auc_selection2 = evaluations.eval_auc(preds.reshape((-1,1)), targets)\n",
    "                val = evaluations.eval_crit(model, valid_loader, crit, device=device)\n",
    "                ledger['internal'].append((step-1, val))\n",
    "                ledger['val_auc'].append((step-1, auc_selection, auc_selection2))\n",
    "\n",
    "                print(f'step {step} ->, train: {train_loss:.3f},  auc: {auc_selection:.3f}, auc2: {auc_selection2:.3f}') # FULL: \n",
    "\n",
    "            if (step % save_intervall) == 0:\n",
    "                torch.save({\n",
    "                        \"step\": step,\n",
    "                        \"model\": model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                    }, \n",
    "                    os.path.join(p.computation['model_out'], f'step{step:05d}.pt')\n",
    "                )\n",
    "                json.dump(ledger, open(os.path.join(p.computation['model_out'], 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        \"step\": step,\n",
    "        \"model\": model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "        \"optim\" : optim.state_dict(),\n",
    "    }, \n",
    "    os.path.join(p.computation['model_out'], f'step{step:05d}.pt')\n",
    ")\n",
    "json.dump(ledger, open(os.path.join(p.computation['model_out'], 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdcf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
