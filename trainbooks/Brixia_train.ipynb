{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torchvision.models\n",
    "import hashlib, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78800357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/jsoelter/Code/ChestImageAI/utils/')\n",
    "sys.path.append('/home/users/jsoelter/Code/big_transfer/')\n",
    "\n",
    "import data_loader, evaluations, model_setup, sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e791155",
   "metadata": {},
   "source": [
    "### Parameter managment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ec6c5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "defaults = {} # default parameter, will be overwritten if set explicilty in this notebooke\n",
    "overwrites = {} # parameter that /OVERWRITE all parameters set in this notebook. Usefull if Notebook is executed externally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sacred.ParameterStore(overwrites=overwrites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe5d82",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.data_setup = dict(\n",
    "    data = {\n",
    "        'global_score': True, \n",
    "        'include_meta': [],\n",
    "        'subset': {'View' : ['AP']}, # Define subsetting of data\n",
    "        #'equalize': None\n",
    "        'val_conf': {\n",
    "                'salt': '40',\n",
    "                'fraction': 0.05,\n",
    "            }\n",
    "    }, \n",
    "    prepro = [\n",
    "        ('ToTensor', {}),\n",
    "        ('Resize', {\n",
    "            'size': 440 #smaller edege mapped to x\n",
    "        })\n",
    "    ],\n",
    "    prepro_dynamic = [\n",
    "        ('Normalize', {\n",
    "            'mean': [0.485, 0.456, 0.406], \n",
    "            #'mean': (0.5, 0.5, 0.5),\n",
    "            'std': [0.229, 0.224, 0.225]  \n",
    "            #'std': (0.5, 0.5, 0.5)\n",
    "        }),\n",
    "    ],\n",
    "    train_aug = [\n",
    "        ('GaussianBlur', {\n",
    "           'kernel_size': 5,\n",
    "            'sigma': (0.1, 1)\n",
    "        }),\n",
    "        ('RandomRotation', {\n",
    "            'degrees': 10\n",
    "        }),\n",
    "        ('CenterCrop', {\n",
    "            'size': (400, 400)\n",
    "        }),\n",
    "        ('RandomCrop', {\n",
    "            'size': (384, 384)\n",
    "        }),\n",
    "\n",
    "    ],\n",
    "    test_aug = [\n",
    "        ('CenterCrop', {\n",
    "            'size': (384, 384)\n",
    "        }),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = data_loader.transform_pipeline_from_dict(p.data_setup['prepro'])\n",
    "train_aug = data_loader.transform_pipeline_from_listdict(p.data_setup, ['prepro_dynamic', 'train_aug'])\n",
    "test_aug = data_loader.transform_pipeline_from_listdict(p.data_setup, ['prepro_dynamic', 'test_aug'])\n",
    "\n",
    "train_data = data_loader.BrixiaData(transform=train_aug, deterministic_transform=prepro, **p.data_setup['data'], cache={})\n",
    "val_data = data_loader.BrixiaData(transform=test_aug, deterministic_transform=prepro, **p.data_setup['data'], cache={}, validation=True)\n",
    "\n",
    "#external_valid_data = data_loader.BrixiaData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c312",
   "metadata": {},
   "source": [
    "#### Visualization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_num = np.random.randint(0, 2000, 4)\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize = (20,5))\n",
    "fig.suptitle('raw images')\n",
    "for ix, n in enumerate(im_num):\n",
    "    row = train_data.meta_df.loc[n]\n",
    "    dcm = data_loader.pydicom.dcmread(os.path.join(train_data.datapath, 'dicom_clean', row.Filename))\n",
    "    axes[ix].imshow(dcm.pixel_array, cmap=plt.cm.Spectral_r)\n",
    "    axes[ix].set_title( f'{int(row.BrixiaScore):06d}')\n",
    "    \n",
    "fig, axes = plt.subplots(1,4,figsize = (20,5))\n",
    "fig.suptitle('train patches rand aug 1')\n",
    "for ix, n in enumerate(im_num):\n",
    "    im, tar, meta = train_data[n]\n",
    "    axes[ix].imshow(im[0], cmap=plt.cm.Spectral_r)\n",
    "    \n",
    "fig, axes = plt.subplots(1,4,figsize = (20,5))\n",
    "fig.suptitle('train patches rand aug 2')\n",
    "for ix, n in enumerate(im_num):\n",
    "    im, tar, meta = train_data[n]\n",
    "    axes[ix].imshow(im[0], cmap=plt.cm.Spectral_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdba6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize = (20,5))\n",
    "im_num = np.random.randint(0, 200, 4)\n",
    "for ix, n in enumerate(im_num):\n",
    "    im, tar, meta = val_data[n]\n",
    "    axes[ix].imshow(im[0], cmap=plt.cm.Spectral_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a537d",
   "metadata": {},
   "source": [
    "#### Caching for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_folder = os.path.join('/work/projects/covid19_dv/models/brixia/cache')\n",
    "train_data.preload(cache_folder)\n",
    "val_data.preload(cache_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3e3d7",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e448961",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.batch = dict(\n",
    "    real_batch_size = 16, #512, #64, #128\n",
    "    batch_split = 1 #16 # #number of forward pathes before optimization is performed \n",
    ")\n",
    "data_batch_size = int(p.batch['real_batch_size']/p.batch['batch_split'])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=data_batch_size, num_workers=8, shuffle=True, drop_last=False)\n",
    "valid_int_loader = torch.utils.data.DataLoader(val_data, batch_size=data_batch_size, num_workers=8, shuffle=False)\n",
    "#valid_ext_loader = torch.utils.data.DataLoader(external_valid_data, batch_size=16, num_workers=8)\n",
    "\n",
    "len(train_loader)/p.batch['batch_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "#timm.list_models('*efficient*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82a3bf",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f33568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(model_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.model_dict =  dict(\n",
    "    #architecture = 'BiT-M-R50x3',\n",
    "    #architecture = 'resnet18',\n",
    "    architecture = 'timm-resnet101d',\n",
    "    num_classes = 1,\n",
    "    #num_heads=6,\n",
    "    pretrained = 'imagenet', \n",
    "    #pretrained = '/home/users/jsoelter/models/chexpert/fullmeta_503_consolidation_new/step00200.pt', #None, #'imagenet','imagenet', #\n",
    "    fresh_head_weights = True,\n",
    "    num_meta=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.computation = {\n",
    "    'model_out': '/work/projects/covid19_dv/models/brixia/jan/global/onlyAP',\n",
    "    'device': \"cuda:0\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(p.computation['model_out']):\n",
    "    os.makedirs(p.computation['model_out'])\n",
    "\n",
    "model = model_setup.instantiate_model(**p.model_dict)\n",
    "\n",
    "saved_models = glob.glob(os.path.join(p.computation['model_out'], 'step*.pt'))\n",
    "if not saved_models:\n",
    "    checkpoint = None\n",
    "    ledger = collections.defaultdict(list)\n",
    "    step = 0\n",
    "else:\n",
    "    last_model = np.sort(saved_models)[-1]\n",
    "    print(f\"Resume training for saved model '{last_model}'\")\n",
    "    checkpoint = torch.load(last_model, map_location=\"cpu\")\n",
    "    re_keyed = {k.split('module.')[-1]: v for k, v in checkpoint['model'].items()}\n",
    "    model.load_state_dict(re_keyed)\n",
    "    \n",
    "    ledger = json.load(open(os.path.join(p.computation['model_out'], 'train_ledger.json')))\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    \n",
    "# Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "# Only good if sizes stay the same within the main loop!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#model = torch.nn.DataParallel(model)\n",
    "device = p.computation['device']\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ecc0b",
   "metadata": {},
   "source": [
    "### Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = 'Adam' #'Adam'#\n",
    "p.opt = {\n",
    "    #'class': 'SGD',\n",
    "    'class': 'Adam',\n",
    "    'param': dict(\n",
    "        lr = 1E-4,\n",
    "        #momentum=0.9,\n",
    "        #nesterov = True,\n",
    "        weight_decay = 1E-3\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = getattr(torch.optim, p.opt['class'])(model.parameters(), **p.opt['param'])\n",
    "if  checkpoint is not None:\n",
    "    optim.load_state_dict(checkpoint[\"optim\"])\n",
    "else:\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ee618",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.scheduler = {\n",
    "    #'supports': [300, 20*len(train_loader), int(40*len(train_loader)), int(60*len(train_loader)), int(80*len(train_loader))]\n",
    "    #'supports':  [100, 300, 500, 700, 800] \n",
    "    #'supports': [300, 1000, 2000, 4000, 6000, 8000]\n",
    "    #'supports': [300, 600, 1500, 3000]\n",
    "    'supports': [300, 1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "print(p.scheduler['supports'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d595b",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.loss = {\n",
    "    #'class': 'L1Loss'\n",
    "    'class': 'MSELoss'\n",
    "    #'class': 'CrossEntropyLoss'\n",
    "}\n",
    "\n",
    "base_loss = getattr(torch.nn, p.loss['class'])(**p.loss.get('param', {}), reduction='sum')\n",
    "#crit = lambda x, y: (base_loss(torch.transpose(x, 1,2), y.long()), np.sum(y.shape))\n",
    "crit = lambda x, y: (base_loss(x, y), np.sum(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c51de3",
   "metadata": {},
   "source": [
    "### Initial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Crit: {evaluations.eval_crit(model, valid_int_loader, crit, device=device):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f051e",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec429c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intervall = 50\n",
    "save_intervall = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c166e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steps = 0\n",
    "batch_loss, batch_samples = 0, 0\n",
    "lr = p.opt['param']['lr']\n",
    "\n",
    "train_setup = ledger.setdefault('train_setup', {})\n",
    "train_setup[step] = {\n",
    "    'setup': p.params\n",
    "}\n",
    "\n",
    "while lr:\n",
    "    for x, y, m in train_loader:\n",
    "        \n",
    "        _ = model.train()\n",
    "\n",
    "        # Update learning-rate, including stop training if over.\n",
    "        lr = model_setup.get_lr(step, supports=p.scheduler['supports'], base_lr=p.opt['param']['lr'])\n",
    "        if lr is None: break\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        \n",
    "        # Schedule sending to GPU(s)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        if getattr(model, 'meta_injection', None):\n",
    "            m = m.to(device, non_blocking=True)\n",
    "            logits = model(x, m)\n",
    "        else:\n",
    "            logits = model(x)            \n",
    "        loss, n_samples = crit(logits, y)\n",
    "        if loss != 0:\n",
    "            # Accumulate grads\n",
    "            (loss / p.batch['batch_split'] / n_samples).backward()\n",
    "\n",
    "        batch_loss += float(loss.data.cpu().numpy())  # Also ensures a sync point.\n",
    "        batch_samples += n_samples\n",
    "\n",
    "        accum_steps += 1\n",
    "\n",
    "        # Update params\n",
    "        if accum_steps == p.batch['batch_split']:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            train_loss = batch_loss/batch_samples\n",
    "            ledger['train_loss'].append(train_loss)\n",
    "            batch_loss, batch_samples = 0, 0\n",
    "            ledger['lr'].append(lr)\n",
    "            step += 1\n",
    "            accum_steps = 0\n",
    "            \n",
    "            # Evaluate \n",
    "            if (step % eval_intervall) == 0:\n",
    "                val = evaluations.eval_crit(model, valid_int_loader, crit, device=device)\n",
    "                ledger['internal'].append((step-1, val))\n",
    "                print(f'step {step} -> train: {train_loss:.3f},  val: {val:.3f}') \n",
    "\n",
    "            if (step % save_intervall) == 0:\n",
    "                torch.save({\n",
    "                        \"step\": step,\n",
    "                        \"model\": model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "                        \"optim\": optim.state_dict(),\n",
    "                    }, \n",
    "                    os.path.join(p.computation['model_out'], f'step{step:05d}.pt')\n",
    "                )\n",
    "                json.dump(ledger, open(os.path.join(p.computation['model_out'], 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a79fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        \"step\": step,\n",
    "        \"model\": model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "    },\n",
    "    os.path.join(p.computation['model_out'], f'step{step:05d}.pt')\n",
    ")\n",
    "json.dump(ledger, open(os.path.join(p.computation['model_out'], 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b880b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
