{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/jsoelter/Code/ChestImageAI/utils/')\n",
    "sys.path.append('/home/users/jsoelter/Code/big_transfer/')\n",
    "\n",
    "import data_loader, evaluations, model_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = torchvision.models.densenet121(pretrained=True)\n",
    "backbone = original_model.features\n",
    "features = self.backbone(x)\n",
    "out = F.relu(features, inplace=True)\n",
    "out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "out = torch.flatten(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiT-M-R101x1.npz  BiT-M-R50x1.npz    BiT-M-R50x3.npz\n",
      "BiT-M-R101x3.npz  BiT-M-R50x1.npz.1\n"
     ]
    }
   ],
   "source": [
    "ls ~/models/BiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "relu(input, inplace=False) -> Tensor\n",
       "\n",
       "Applies the rectified linear unit function element-wise. See\n",
       ":class:`~torch.nn.ReLU` for more details.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wget https://storage.googleapis.com/bit_models/BiT-M-R50x1.{npz|h5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, name, flat=True):\n",
    "        \n",
    "        if name == 'densenet121':\n",
    "            model = torchvision.models.densenet121(pretrained= (pretrained == 'imagenet'))\n",
    "            self.backbone = model.features\n",
    "            self.flatten = torch.nn.ReLU\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6144"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.body.block4.unit03.conv3.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bit_pytorch.models import ResNetV2, KNOWN_MODELS as BiTModels\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "\n",
    "class ResNetV2_feature_extractor(ResNetV2):\n",
    "    \n",
    "    def __init__(self, block_units, width_factor, head_size=21843, zero_head=False):\n",
    "        super().__init__(block_units, width_factor, head_size=21843, zero_head=False)\n",
    "        self.pre_head = nn.Sequential(OrderedDict([\n",
    "            ('gn', nn.GroupNorm(32, 2048*width_factor)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            ('avg', nn.AdaptiveAvgPool2d(output_size=1))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pre_head(self.body(self.root(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BiT_MetaMixin(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_meta_data, backbone):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.classifier =nn.Conv2d(self.backbone.body.block4.unit03.conv3.out_channels+num_meta_data, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "        \n",
    "\n",
    "    def forward(self, im, meta_data):\n",
    "        \n",
    "        img_feat = self.backbone(im)\n",
    "        feat = torch.cat([img_feat, meta_data], 1)      \n",
    "        pred = self.classifier(feat)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "m = BiT_MetaMixin(1, ResNetV2_feature_extractor([3, 4, 6, 3], 3, head_size=1, zero_head=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict =  dict(\n",
    "    architecture = 'BiT-M-R50x3',\n",
    "    #architecture = 'densenet121',\n",
    "    num_classes = 5,\n",
    "    pretrained = None, #'imagenet', #'/home/users/jsoelter/models/chexpert/fullmeta_503/step05000.pt',\n",
    "    fresh_head_weights = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = '/home/users/jsoelter/models/chexpert/densenet/test'\n",
    "\n",
    "if not os.path.exists(model_out):\n",
    "    os.makedirs(model_out)\n",
    "\n",
    "model = model_setup.instantiate_model(**model_dict)\n",
    "\n",
    "saved_models = glob.glob(os.path.join(model_out, 'step*.pt'))\n",
    "if not saved_models:\n",
    "    checkpoint = None\n",
    "    ledger = collections.defaultdict(list)\n",
    "    ledger['model'] = model_dict\n",
    "    step = 0\n",
    "else:\n",
    "    last_model = np.sort(saved_models)[-1]\n",
    "    print(f\"Resume training for saved model '{last_model}'\")\n",
    "    checkpoint = torch.load(last_model, map_location=\"cpu\")\n",
    "    re_keyed = {k.split('module.')[-1]: v for k, v in checkpoint['model'].items()}\n",
    "    model.load_state_dict(re_keyed)\n",
    "    \n",
    "    ledger = json.load(open(os.path.join(model_out, 'train_ledger.json')))\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    \n",
    "# Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "# Only good if sizes stay the same within the main loop!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_setup = dict(\n",
    "    include_meta = [],\n",
    "    #include_meta = ['Sex', 'AP/PA', 'Frontal/Lateral'],\n",
    "    label_value_map = {\n",
    "       0: 0.01,\n",
    "       'nan': 0.05,\n",
    "       -1.: 0.5,\n",
    "       1: 0.98\n",
    "    },\n",
    "    fill_hierachy = {\n",
    "        #'Enlarged Cardiomediastinum': ['Cardiomegaly'],\n",
    "        #'Consolidation': ['Pneumonia'],\n",
    "        #'Lung Opacity': ['Edema', 'Pneumonia', 'Consolidation', 'Lung Lesion', 'Atelectasis']\n",
    "    },\n",
    "    labels = ['Cardiomegaly', 'Edema',  'Consolidation', 'Atelectasis', 'Pleural Effusion'],\n",
    "    subset = {}, # Define subsetting of data\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    ('ToPILImage', {}),\n",
    "    ('Resize', {\n",
    "        'size': 136 #smaller edege mapped to x\n",
    "    }),\n",
    "    #('Resize', {\n",
    "    #    'size': 544\n",
    "    #}),\n",
    "    ('RandomRotation', {\n",
    "        'degrees': 5\n",
    "    }),    \n",
    "    ('RandomCrop', {\n",
    "        'size': (128, 128)\n",
    "    }),\n",
    "    ('ToTensor', {}),\n",
    "    ('Normalize', {\n",
    "        'mean': [0.485, 0.456, 0.406], #'mean': (0.5, 0.5, 0.5),\n",
    "        'std': [0.229, 0.224, 0.225]  #(0.5, 0.5, 0.5)\n",
    "    }),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 entries\n",
      "Removed 0 entries\n"
     ]
    }
   ],
   "source": [
    "preprocess = data_loader.transform_pipeline_from_dict(transforms)\n",
    "\n",
    "data = data_loader.ChexpertData('CheXpert-v1.0/train.csv', transform=preprocess, **data_setup)\n",
    "internal_valid_data, train_data = torch.utils.data.random_split(data, [1000, len(data)-1000], generator=torch.Generator().manual_seed(42))\n",
    "external_valid_data = data_loader.ChexpertData('CheXpert-v1.0/valid.csv', transform=preprocess, \n",
    "                                               labels=data_setup['labels'], include_meta=data_setup['include_meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223414, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64540"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.meta_df.Path.str.split('/').str.get(-3).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch_size = 64#128\n",
    "batch_split = 1 #8 #number of forward pathes before optimization is performed \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True, drop_last=False)\n",
    "\n",
    "valid_int_loader = torch.utils.data.DataLoader(internal_valid_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True)\n",
    "valid_ext_loader = torch.utils.data.DataLoader(external_valid_data, batch_size=16, num_workers=8)\n",
    "\n",
    "len(train_loader)/batch_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'Adam'#'SGD'\n",
    "opt_param = dict(\n",
    "    lr = 1E-4,\n",
    "    #momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "#optim = torch.optim.SGD([p for n,p in model.named_parameters() if 'head' in n], lr=0.003, momentum=0.9)\n",
    "optim = getattr(torch.optim, opt)(model.parameters(), **opt_param)\n",
    "if  checkpoint is not None:\n",
    "    optim.load_state_dict(checkpoint[\"optim\"])\n",
    "else:\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = [-1, 2*len(train_loader), int(3*len(train_loader)), int(4*len(train_loader))]#[3000, 7000, 9000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on = None #['Cardiomegaly'] #,  'Consolidation' 'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema', 'Support Devices',  'AP/PA', 'Sex', 'Frontal/Lateral'] #,\n",
    "train_cols = [i in train_on for i in data.targets] if train_on else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = model_setup.maskedBCE(train_cols, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "print(f'AUC on {train_on}: {evaluations.eval_auc(preds, targets, train_cols):.3f}')\n",
    "print(f'Crit on {train_on}: {evaluations.eval_crit(model, valid_int_loader, crit):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intervall = 50\n",
    "save_intervall = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steps = 0\n",
    "batch_loss, batch_samples = 0, 0\n",
    "lr = opt_param['lr']\n",
    "\n",
    "train_setup = ledger.setdefault('train_setup', {})\n",
    "train_setup[step] = {\n",
    "    'transforms': transforms,\n",
    "    'data_setup': data_setup,\n",
    "    'optimizer': {\n",
    "        'name': opt,\n",
    "        'param': opt_param\n",
    "    },\n",
    "    'real_batch_size': real_batch_size,\n",
    "    'batch_split': batch_split\n",
    "}\n",
    "\n",
    "while lr:\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        _ = model.train()\n",
    "        \n",
    "        # Schedule sending to GPU(s)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Update learning-rate, including stop training if over.\n",
    "        lr = model_setup.get_lr(step, supports=supports, base_lr=opt_param['lr'])\n",
    "        if lr is None: break\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        logits = model(x)\n",
    "        loss, n_samples = crit(logits, y)\n",
    "        if loss != 0:\n",
    "            # Accumulate grads\n",
    "            (loss / batch_split / n_samples).backward()\n",
    "\n",
    "        batch_loss += float(loss.data.cpu().numpy())  # Also ensures a sync point.\n",
    "        batch_samples += n_samples.cpu().numpy()\n",
    "\n",
    "        accum_steps += 1\n",
    "\n",
    "        # Update params\n",
    "        if accum_steps == batch_split:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            train_loss = batch_loss/batch_samples\n",
    "            ledger['train_loss'].append(train_loss)\n",
    "            batch_loss, batch_samples = 0, 0\n",
    "            ledger['lr'].append(lr)\n",
    "            step += 1\n",
    "            accum_steps = 0\n",
    "            \n",
    "            # Evaluate \n",
    "            if (step % eval_intervall) == 0:\n",
    "                preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "                auc_selection = evaluations.eval_auc(preds, targets, train_cols)\n",
    "                benchmark_cols = [i in ['Consolidation',  'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema'] for i in data.targets] \n",
    "                auc_bench = evaluations.eval_auc(preds, targets, benchmark_cols)\n",
    "                ledger['external'].append((step-1, auc_selection))\n",
    "                val = evaluations.eval_crit(model, valid_int_loader, crit)\n",
    "                ledger['internal'].append((step-1, val))\n",
    "                print(f'step {step} ->, train: {train_loss:.3f}, val internal : {val:.3f}, auc select: {auc_selection:.3f}, auc bench: {auc_bench:.3f}') # FULL: \n",
    "\n",
    "            if (step % save_intervall) == 0:\n",
    "                torch.save({\n",
    "                        \"step\": step,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optim\" : optim.state_dict(),\n",
    "                    }, \n",
    "                    os.path.join(model_out, f'step{step:05d}.pt')\n",
    "                )\n",
    "                json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        \"step\": step,\n",
    "        \"model\": model.module.state_dict(),\n",
    "        \"optim\" : optim.state_dict(),\n",
    "    }, \n",
    "    os.path.join(model_out, f'step{step:05d}.pt')\n",
    ")\n",
    "json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger['model'] = model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
