{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import json_tricks as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/jsoelter/Code/ChestImageAI/utils/')\n",
    "sys.path.append('/home/users/jsoelter/Code/big_transfer/')\n",
    "\n",
    "import data_loader, evaluations, model_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict =  dict(\n",
    "    architecture = 'BiT-M-R50x3',\n",
    "    #architecture = 'densenet121',\n",
    "    num_classes = 5,\n",
    "    pretrained = 'imagenet', #'/home/users/jsoelter/models/chexpert/fullmeta_503/step05000.pt',\n",
    "    fresh_head_weights = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIT\n"
     ]
    }
   ],
   "source": [
    "model_out = '/home/users/jsoelter/models/chexpert/dense/april_base7'\n",
    "\n",
    "if not os.path.exists(model_out):\n",
    "    os.makedirs(model_out)\n",
    "\n",
    "model = model_setup.instantiate_model(**model_dict)\n",
    "\n",
    "saved_models = glob.glob(os.path.join(model_out, 'step*.pt'))\n",
    "if not saved_models:\n",
    "    checkpoint = None\n",
    "    ledger = collections.defaultdict(list)\n",
    "    ledger['model'] = model_dict\n",
    "    step = 0\n",
    "else:\n",
    "    last_model = np.sort(saved_models)[-1]\n",
    "    print(f\"Resume training for saved model '{last_model}'\")\n",
    "    checkpoint = torch.load(last_model, map_location=\"cpu\")\n",
    "    re_keyed = {k.split('module.')[-1]: v for k, v in checkpoint['model'].items()}\n",
    "    model.load_state_dict(re_keyed)\n",
    "    \n",
    "    ledger = json.load(open(os.path.join(model_out, 'train_ledger.json')))\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    \n",
    "# Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "# Only good if sizes stay the same within the main loop!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_setup = dict(\n",
    "    include_meta = [],\n",
    "    #include_meta = ['Sex', 'AP/PA', 'Frontal/Lateral'],\n",
    "    label_value_map = {\n",
    "       0: 0.01,\n",
    "       'nan': 0.05,\n",
    "       -1.: 0.5,\n",
    "       1: 0.9\n",
    "    },\n",
    "    fill_hierachy = {\n",
    "        #'Enlarged Cardiomediastinum': ['Cardiomegaly'],\n",
    "        #'Consolidation': ['Pneumonia'],\n",
    "        #'Lung Opacity': ['Edema', 'Pneumonia', 'Consolidation', 'Lung Lesion', 'Atelectasis']\n",
    "    },\n",
    "    labels = ['Cardiomegaly', 'Edema',  'Consolidation', 'Atelectasis', 'Pleural Effusion'],\n",
    "    subset = {}, # Define subsetting of data\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    ('ToPILImage', {}),\n",
    "    #('Resize', {\n",
    "    #    'size': 320 #smaller edege mapped to x\n",
    "    #}),\n",
    "    ('Resize', {\n",
    "        'size': 342\n",
    "    }),\n",
    "    ('RandomRotation', {\n",
    "        'degrees': 5\n",
    "    }),    \n",
    "    ('RandomCrop', {\n",
    "        'size': (320, 320)\n",
    "    }),\n",
    "    ('ToTensor', {}),\n",
    "    ('Normalize', {\n",
    "        #'mean': [0.485, 0.456, 0.406], \n",
    "        'mean': (0.5, 0.5, 0.5),\n",
    "        #'std': [0.229, 0.224, 0.225]  \n",
    "        'std': (0.5, 0.5, 0.5)\n",
    "    }),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 entries\n",
      "Removed 0 entries\n"
     ]
    }
   ],
   "source": [
    "preprocess = data_loader.transform_pipeline_from_dict(transforms)\n",
    "\n",
    "data = data_loader.ChexpertData('CheXpert-v1.0/train.csv', transform=preprocess, **data_setup)\n",
    "internal_valid_data, train_data = torch.utils.data.random_split(data, [1000, len(data)-1000], generator=torch.Generator().manual_seed(42))\n",
    "external_valid_data = data_loader.ChexpertData('CheXpert-v1.0/valid.csv', transform=preprocess, \n",
    "                                               labels=data_setup['labels'], include_meta=data_setup['include_meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868.8125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_batch_size = 256#128\n",
    "batch_split = 16 #8 #number of forward pathes before optimization is performed \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True, drop_last=False)\n",
    "\n",
    "valid_int_loader = torch.utils.data.DataLoader(internal_valid_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True)\n",
    "valid_ext_loader = torch.utils.data.DataLoader(external_valid_data, batch_size=16, num_workers=8)\n",
    "\n",
    "len(train_loader)/batch_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'Adam'\n",
    "#opt = 'SGD'\n",
    "opt_param = dict(\n",
    "    lr = 3E-4,\n",
    "    #momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "#optim = torch.optim.SGD([p for n,p in model.named_parameters() if 'head' in n], lr=0.003, momentum=0.9)\n",
    "optim = getattr(torch.optim, opt)(model.parameters(), **opt_param)\n",
    "if  checkpoint is not None:\n",
    "    optim.load_state_dict(checkpoint[\"optim\"])\n",
    "else:\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = [100, 2000, 3000, 3500, 4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on = None #['Cardiomegaly'] #,  'Consolidation' 'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema', 'Support Devices',  'AP/PA', 'Sex', 'Frontal/Lateral'] #,\n",
    "train_cols = [i in train_on for i in data.targets] if train_on else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting = 'equal'\n",
    "\n",
    "if weighting == 'equal':\n",
    "    w_pos = [] \n",
    "    for c in data_setup['labels']:\n",
    "        m = (train_data.dataset.meta_df[c]>0.5)\n",
    "        w_pos.append(np.logical_not(m).sum()/m.sum())\n",
    "else:\n",
    "    w_pos = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluations' from '/home/users/jsoelter/Code/ChestImageAI/utils/evaluations.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(model_setup)\n",
    "importlib.reload(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = model_setup.maskedBCE(train_cols, device, pos_weight=w_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on None: 0.500\n",
      "Crit on None: 0.693\n"
     ]
    }
   ],
   "source": [
    "preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "print(f'AUC on {train_on}: {evaluations.eval_auc(preds, targets, train_cols):.3f}')\n",
    "print(f'Crit on {train_on}: {evaluations.eval_crit(model, valid_int_loader, crit):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intervall = 50\n",
    "save_intervall = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50 ->, train: 0.666, val internal : 0.674, auc bench: 0.76, 0.78, 0.85, 0.79, 0.81\n",
      "step 100 ->, train: 0.656, val internal : 0.654, auc bench: 0.72, 0.77, 0.88, 0.81, 0.82\n",
      "step 150 ->, train: 0.644, val internal : 0.657, auc bench: 0.78, 0.81, 0.90, 0.81, 0.81\n",
      "step 200 ->, train: 0.668, val internal : 0.649, auc bench: 0.78, 0.83, 0.90, 0.59, 0.82\n",
      "step 250 ->, train: 0.652, val internal : 0.642, auc bench: 0.76, 0.83, 0.89, 0.68, 0.84\n",
      "step 300 ->, train: 0.647, val internal : 0.628, auc bench: 0.81, 0.84, 0.93, 0.62, 0.85\n",
      "step 350 ->, train: 0.620, val internal : 0.624, auc bench: 0.80, 0.86, 0.92, 0.84, 0.88\n",
      "step 400 ->, train: 0.611, val internal : 0.625, auc bench: 0.78, 0.87, 0.91, 0.77, 0.89\n"
     ]
    }
   ],
   "source": [
    "accum_steps = 0\n",
    "batch_loss, batch_samples = 0, 0\n",
    "lr = opt_param['lr']\n",
    "\n",
    "train_setup = ledger.setdefault('train_setup', {})\n",
    "train_setup[step] = {\n",
    "    'transforms': transforms,\n",
    "    'data_setup': data_setup,\n",
    "    'optimizer': {\n",
    "        'name': opt,\n",
    "        'param': opt_param\n",
    "    },\n",
    "    'real_batch_size': real_batch_size,\n",
    "    'batch_split': batch_split\n",
    "}\n",
    "\n",
    "while lr:\n",
    "    for x, y, _ in train_loader:\n",
    "        \n",
    "        _ = model.train()\n",
    "        \n",
    "        # Schedule sending to GPU(s)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Update learning-rate, including stop training if over.\n",
    "        lr = model_setup.get_lr(step, supports=supports, base_lr=opt_param['lr'])\n",
    "        if lr is None: break\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        logits = model(x)\n",
    "        loss, n_samples = crit(logits, y)\n",
    "        if loss != 0:\n",
    "            # Accumulate grads\n",
    "            (loss / batch_split / n_samples).backward()\n",
    "\n",
    "        batch_loss += float(loss.data.cpu().numpy())  # Also ensures a sync point.\n",
    "        batch_samples += n_samples.cpu().numpy()\n",
    "\n",
    "        accum_steps += 1\n",
    "\n",
    "        # Update params\n",
    "        if accum_steps == batch_split:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            train_loss = batch_loss/batch_samples\n",
    "            ledger['train_loss'].append(train_loss)\n",
    "            batch_loss, batch_samples = 0, 0\n",
    "            ledger['lr'].append(lr)\n",
    "            step += 1\n",
    "            accum_steps = 0\n",
    "            \n",
    "            # Evaluate \n",
    "            if (step % eval_intervall) == 0:\n",
    "                preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "                #auc_selection = evaluations.eval_auc(preds, targets, train_cols)\n",
    "                #benchmark_cols = [i in ['Consolidation',  'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema'] for i in data.targets] \n",
    "                auc_bench = evaluations.eval_auc_percol(preds, targets)\n",
    "                ledger['external'].append([step-1].extend(auc_bench))\n",
    "                val = evaluations.eval_crit(model, valid_int_loader, crit)\n",
    "                ledger['internal'].append((step-1, val.cpu().numpy()))\n",
    "                score = ', '.join([f'{a:.2f}' for a in auc_bench])\n",
    "                print(f'step {step} ->, train: {train_loss:.3f}, val internal : {val:.3f}, auc bench: ' + score) # FULL: \n",
    "\n",
    "            if (step % save_intervall) == 0:\n",
    "                torch.save({\n",
    "                        \"step\": step,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optim\" : optim.state_dict(),\n",
    "                    }, \n",
    "                    os.path.join(model_out, f'step{step:05d}.pt')\n",
    "                )\n",
    "                json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        \"step\": step,\n",
    "        \"model\": model.module.state_dict(),\n",
    "        \"optim\" : optim.state_dict(),\n",
    "    }, \n",
    "    os.path.join(model_out, f'step{step:05d}.pt')\n",
    ")\n",
    "_ = json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
