{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/jsoelter/Code/ChestImageAI/utils/')\n",
    "sys.path.append('/home/users/jsoelter/Code/big_transfer/')\n",
    "\n",
    "import data_loader, evaluations, model_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict =  dict(\n",
    "    #architecture = 'BiT-M-R50x3',\n",
    "    architecture = 'densenet121',\n",
    "    num_classes = 5,\n",
    "    pretrained = 'imagenet', #'/home/users/jsoelter/models/chexpert/fullmeta_503/step05000.pt',\n",
    "    fresh_head_weights = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = '/home/users/jsoelter/models/chexpert/densenet/pain_lowres'\n",
    "\n",
    "if not os.path.exists(model_out):\n",
    "    os.makedirs(model_out)\n",
    "\n",
    "model = model_setup.instantiate_model(**model_dict)\n",
    "\n",
    "saved_models = glob.glob(os.path.join(model_out, 'step*.pt'))\n",
    "if not saved_models:\n",
    "    checkpoint = None\n",
    "    ledger = collections.defaultdict(list)\n",
    "    ledger['model'] = model_dict\n",
    "    step = 0\n",
    "else:\n",
    "    last_model = np.sort(saved_models)[-1]\n",
    "    print(f\"Resume training for saved model '{last_model}'\")\n",
    "    checkpoint = torch.load(last_model, map_location=\"cpu\")\n",
    "    re_keyed = {k.split('module.')[-1]: v for k, v in checkpoint['model'].items()}\n",
    "    model.load_state_dict(re_keyed)\n",
    "    \n",
    "    ledger = json.load(open(os.path.join(model_out, 'train_ledger.json')))\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    \n",
    "# Lets cuDNN benchmark conv implementations and choose the fastest.\n",
    "# Only good if sizes stay the same within the main loop!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_setup = dict(\n",
    "    include_meta = [],\n",
    "    #include_meta = ['Sex', 'AP/PA', 'Frontal/Lateral'],\n",
    "    label_value_map = {\n",
    "       0: 0.05,\n",
    "       'nan': 0.1,\n",
    "       -1.: 0.5,\n",
    "       1: 0.9\n",
    "    },\n",
    "    fill_hierachy = {\n",
    "        #'Enlarged Cardiomediastinum': ['Cardiomegaly'],\n",
    "        #'Consolidation': ['Pneumonia'],\n",
    "        #'Lung Opacity': ['Edema', 'Pneumonia', 'Consolidation', 'Lung Lesion', 'Atelectasis']\n",
    "    },\n",
    "    labels = ['Cardiomegaly', 'Edema',  'Consolidation', 'Atelectasis', 'Pleural Effusion'],\n",
    "    subset = {}, # Define subsetting of data\n",
    ")\n",
    "\n",
    "transforms = {\n",
    "    'ToPILImage': {},\n",
    "    'Resize': {\n",
    "        'size': 272 #smaller edege mapped to x\n",
    "    },\n",
    "    'RandomRotation': {\n",
    "        'degrees': 5\n",
    "    },    \n",
    "    'RandomCrop': {\n",
    "        'size': (256, 256)\n",
    "    },\n",
    "    'Resize': {\n",
    "        'size': 512\n",
    "    },\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\n",
    "        'mean': [0.485, 0.456, 0.406], #'mean': (0.5, 0.5, 0.5),\n",
    "        'std': [0.229, 0.224, 0.225]  #(0.5, 0.5, 0.5)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 entries\n",
      "Removed 0 entries\n"
     ]
    }
   ],
   "source": [
    "preprocess = data_loader.transform_pipeline_from_dict(transforms)\n",
    "\n",
    "data = data_loader.ChexpertData('CheXpert-v1.0/train.csv', transform=preprocess, **data_setup)\n",
    "internal_valid_data, train_data = torch.utils.data.random_split(data, [1000, len(data)-1000], generator=torch.Generator().manual_seed(42))\n",
    "external_valid_data = data_loader.ChexpertData('CheXpert-v1.0/valid.csv', transform=preprocess, \n",
    "                                               labels=data_setup['labels'], include_meta=data_setup['include_meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6951.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_batch_size = 32#128\n",
    "batch_split = 1 #8 #number of forward pathes before optimization is performed \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True, drop_last=False)\n",
    "\n",
    "valid_int_loader = torch.utils.data.DataLoader(internal_valid_data, batch_size=int(real_batch_size/batch_split), num_workers=8, shuffle=True)\n",
    "valid_ext_loader = torch.utils.data.DataLoader(external_valid_data, batch_size=16, num_workers=8)\n",
    "\n",
    "len(train_loader)/batch_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'Adam'#'SGD'\n",
    "opt_param = dict(\n",
    "    lr = 1E-4,\n",
    "    #momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "#optim = torch.optim.SGD([p for n,p in model.named_parameters() if 'head' in n], lr=0.003, momentum=0.9)\n",
    "optim = getattr(torch.optim, opt)(model.parameters(), **opt_param)\n",
    "if  checkpoint is not None:\n",
    "    optim.load_state_dict(checkpoint[\"optim\"])\n",
    "else:\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = [-1, len(train_loader), int(1.5*len(train_loader)), int(2*len(train_loader))]#[3000, 7000, 9000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on = None #['Cardiomegaly'] #,  'Consolidation' 'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema', 'Support Devices',  'AP/PA', 'Sex', 'Frontal/Lateral'] #,\n",
    "train_cols = [i in train_on for i in data.targets] if train_on else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = model_setup.maskedBCE(train_cols, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on None: 0.572\n",
      "Crit on None: 0.780\n"
     ]
    }
   ],
   "source": [
    "preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "print(f'AUC on {train_on}: {evaluations.eval_auc(preds, targets, train_cols):.3f}')\n",
    "print(f'Crit on {train_on}: {evaluations.eval_crit(model, valid_int_loader, crit):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intervall = 50\n",
    "save_intervall = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 ->, train: 0.593, val internal : 0.568, auc select: 0.727, auc bench: 0.727\n",
      "step 150 ->, train: 0.552, val internal : 0.557, auc select: 0.688, auc bench: 0.688\n",
      "step 200 ->, train: 0.547, val internal : 0.556, auc select: 0.741, auc bench: 0.741\n",
      "step 250 ->, train: 0.583, val internal : 0.551, auc select: 0.715, auc bench: 0.715\n",
      "step 300 ->, train: 0.547, val internal : 0.552, auc select: 0.742, auc bench: 0.742\n",
      "step 350 ->, train: 0.533, val internal : 0.554, auc select: 0.698, auc bench: 0.698\n",
      "step 400 ->, train: 0.590, val internal : 0.552, auc select: 0.712, auc bench: 0.712\n",
      "step 450 ->, train: 0.524, val internal : 0.550, auc select: 0.727, auc bench: 0.727\n",
      "step 500 ->, train: 0.583, val internal : 0.553, auc select: 0.741, auc bench: 0.741\n",
      "step 550 ->, train: 0.573, val internal : 0.552, auc select: 0.722, auc bench: 0.722\n",
      "step 600 ->, train: 0.508, val internal : 0.551, auc select: 0.747, auc bench: 0.747\n",
      "step 650 ->, train: 0.503, val internal : 0.554, auc select: 0.750, auc bench: 0.750\n",
      "step 700 ->, train: 0.578, val internal : 0.550, auc select: 0.726, auc bench: 0.726\n",
      "step 750 ->, train: 0.572, val internal : 0.553, auc select: 0.777, auc bench: 0.777\n",
      "step 800 ->, train: 0.553, val internal : 0.549, auc select: 0.729, auc bench: 0.729\n",
      "step 850 ->, train: 0.575, val internal : 0.551, auc select: 0.757, auc bench: 0.757\n"
     ]
    }
   ],
   "source": [
    "accum_steps = 0\n",
    "batch_loss, batch_samples = 0, 0\n",
    "\n",
    "train_setup = ledger.setdefault('train_setup', {})\n",
    "train_setup[step] = {\n",
    "    'transforms': transforms,\n",
    "    'data_setup': data_setup,\n",
    "    'optimizer': {\n",
    "        'name': opt,\n",
    "        'param': opt_param\n",
    "    },\n",
    "    'real_batch_size': real_batch_size,\n",
    "    'batch_split': batch_split\n",
    "}\n",
    "\n",
    "while True:\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        _ = model.train()\n",
    "        \n",
    "        # Schedule sending to GPU(s)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Update learning-rate, including stop training if over.\n",
    "        lr = model_setup.get_lr(step, supports=supports, base_lr=opt_param['lr'])\n",
    "        if lr is None:\n",
    "            break\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        logits = model(x)\n",
    "        loss, n_samples = crit(logits, y)\n",
    "        if loss != 0:\n",
    "            # Accumulate grads\n",
    "            (loss / batch_split / n_samples).backward()\n",
    "\n",
    "        batch_loss += float(loss.data.cpu().numpy())  # Also ensures a sync point.\n",
    "        batch_samples += n_samples.cpu().numpy()\n",
    "\n",
    "        accum_steps += 1\n",
    "\n",
    "        # Update params\n",
    "        if accum_steps == batch_split:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            train_loss = batch_loss/batch_samples\n",
    "            ledger['train_loss'].append(train_loss)\n",
    "            batch_loss, batch_samples = 0, 0\n",
    "            ledger['lr'].append(lr)\n",
    "            step += 1\n",
    "            accum_steps = 0\n",
    "            \n",
    "            # Evaluate \n",
    "            if (step % eval_intervall) == 0:\n",
    "                preds, targets = evaluations.batch_prediction(model, valid_ext_loader)\n",
    "                auc_selection = evaluations.eval_auc(preds, targets, train_cols)\n",
    "                benchmark_cols = [i in ['Consolidation',  'Cardiomegaly', 'Atelectasis', 'Pleural Effusion', 'Edema'] for i in data.targets] \n",
    "                auc_bench = evaluations.eval_auc(preds, targets, benchmark_cols)\n",
    "                ledger['external'].append((step-1, auc_selection))\n",
    "                val = evaluations.eval_crit(model, valid_int_loader, crit)\n",
    "                ledger['internal'].append((step-1, val))\n",
    "                print(f'step {step} ->, train: {train_loss:.3f}, val internal : {val:.3f}, auc select: {auc_selection:.3f}, auc bench: {auc_bench:.3f}') # FULL: \n",
    "\n",
    "            if (step % save_intervall) == 0:\n",
    "                torch.save({\n",
    "                        \"step\": step,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optim\" : optim.state_dict(),\n",
    "                    }, \n",
    "                    os.path.join(model_out, f'step{step:05d}.pt')\n",
    "                )\n",
    "                json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        \"step\": step,\n",
    "        \"model\": model.module.state_dict(),\n",
    "        \"optim\" : optim.state_dict(),\n",
    "    }, \n",
    "    os.path.join(model_out, f'step{step:05d}.pt')\n",
    ")\n",
    "json.dump(ledger, open(os.path.join(model_out, 'train_ledger.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger['model'] = model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
